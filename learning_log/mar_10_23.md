# 3/10/23

## Deep Leaning Concepts

### ML boosting algorithms XGBoost, CatBoost and LightGBM

#### Topics Covered Today:

- **What are the _Four Fundamental Questions_ in decision tree based models?**
  1. **What features and cut off to start with?**
    - Goal: To get the most info gain
      - Control how decision tree splits data into like subgroups
    - Regression Trees: MSE
    - Classification Tree
      - Error
      - Entropy
      - Gini Index
  2. **How to split the samples (pre-sorted and histogram, GOSS, Greedy)**
    - **Pre-sorted/Histogram Based:** Sorts and creats histos of values before splitting. Faster splits with less accurate trees
    - **GOSS (Gradient-Based One-Sided Sampling)**: Uses gradient info as measure of the weight for splitting.
      - Powerful and most widely used
      - Keeps large gradients while performing random sampling on instances with small gradients
    - **Greedy Method**: Selects the best split at each step without considering impact on future splits
      - May result in suboptimal trees
  3. **How to grow a tree? (Depth-wise, Leaf-wise, Symmetric)**
    - **Depth-Wise (Level-Wise):** Repeatedly splitting data on feature with highest info gain **_until certain depth reached._** Results in balanced tree structure
    - **Leaf-Wise:** Repeatedly splitting data on feature with highest info gain **_until all lead nodes only contain a single class._** Results in highly unbalanced structures with some branches being much deeper than others.
    - **Symmetric:** Repeatedly splitting data on feature with highest info gain **_until a certain stopping criteria is met_** (eg min number of samples per leaf node). Results in more balanced tree than Leaf-Wise method.
  4. **How to combine trees (Bagging vs Boosting)**
    - **Bagging** consists of creating many copies of the training data (each slightly different than the others) and apply the weak learner to each copy. Then combine them
      - In **Bagging**, the bootstrapped trees are independent from each other
      - Done in parallel
    - **Boosting** is using "original" training data and iteratively creating multiple models by using a weak learner.
      - In **Boosting** each tree is grown using info from previous tree.
      - Done sequentially
- **Evolution of XGBoost**
  - Why we need to go beyond the above.
  - This p[tom,ozes gradient boosting algo through parallel processing, tree-pruning, handling missing values, and regularization to avoid overfitting/bias 
- **XGBoost details**
  - Open source gradient boosting lib focused on efficient and scalable ML algos
  - _eXtreme GB_, the extreme in this refers to the fact algos and methods have been customized to push the limit og GB algos
  - also helps with missing values, feature selection, and model ensembling.
- **LightGBM details**
  - Open source GB library developed my Microsoft.
  - Fast, efficient making it suitable for large scale learning tasks
  - Handles categorical features but requires one-hot encoding, ordinal encoding, or other processing
  - also helps with missing values, feature selection, and model ensembling.

- **CatBoost details**
  - Open source GB dev in 2017.
  - **DESIGNED** to handle categorical data
  - Can handle categorical data without one hot encoding or other processing
  - also helps with missing values, feature selection, and model ensembling.
- **Comparing XGBoost, LightGBM and CatBoost**

![comparison between 3 systems](./assets/Screenshot%202023-03-10%20111424.png)
 
### Take Away
- Look into Kaggle [Competitions](https://www.kaggle.com/competitions)
- LightGBM most scaleable of the three
